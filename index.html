<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head>  <title>Xing SUN's Homepage</title>   <meta http-equiv="content-type" content="text/html;charset=UTF-8">   <style type="text/css">   /*CSS stylesheet is based on killwing's flavored markdown style:https://gist.github.com/2937864*/body{    margin: 0 auto;    font: 13px/1.231 Helvetica, Arial, sans-serif;    color: #444444;    line-height: 1;    max-width: 960px;    padding: 5px;}h1, h2, h3, h4 {    color: #111111;    font-weight: 400;}h1, h2, h3, h4, h5, p {    margin-bottom: 16px;    padding: 0;}h1 {    font-size: 28px;}h2 {    font-size: 22px;    margin: 20px 0 6px;}h3 {    font-size: 21px;}h4 {    font-size: 18px;}h5 {    font-size: 16px;}a {    color: #0099ff;    margin: 0;    padding: 0;    vertical-align: baseline;}a:link,a:visited{ text-decoration:none;}a:hover{ text-decoration:underline;}ul, ol {    padding: 0;    margin: 0;}li {    line-height: 24px;    margin-left: 44px;}li ul, li ul {    margin-left: 24px;}ul, ol {    font-size: 14px;    line-height: 20px;    max-width: 540px;}p {    font-size: 14px;    line-height: 20px;    max-width: 1000px;    margin-top: 3px;}pre {    padding: 0px 4px;    max-width: 1000px;    white-space: pre-wrap;    font-family: Consolas, Monaco, Andale Mono, monospace;    line-height: 1.5;    font-size: 13px;    border: 1px solid #ddd;    background-color: #f7f7f7;    border-radius: 3px;}code {    font-family: Consolas, Monaco, Andale Mono, monospace;    line-height: 1.5;    font-size: 13px;    border: 1px solid #ddd;    background-color: #f7f7f7;    border-radius: 3px;}pre code {    border: 0px;}aside {    display: block;    float: right;    width: 390px;}blockquote{    border-left:.5em solid #40aa53;    padding: 0 2em;    margin-left:0;    max-width: 1200px;}blockquote:nth-of-type(2){    border-left:.5em solid #4114b4;    padding: 0 2em;    margin-left:0;    max-width: 1200px;}blockquote:nth-of-type(3){    border-left:.5em solid #4114b4;    padding: 0 2em;    margin-left:0;    max-width: 1200px;}blockquote:nth-of-type(4){    border-left:.5em solid #4114b4;    padding: 0 2em;    margin-left:0;    max-width: 1200px;}blockquote:nth-of-type(5){    border-left:.5em solid #4114b4;    padding: 0 2em;    margin-left:0;    max-width: 1200px;}blockquote:nth-of-type(6){    border-left:.5em solid #4114b4;    padding: 0 2em;    margin-left:0;    max-width: 1200px;}blockquote:nth-of-type(11){    border-left:.5em solid #4114b4;    padding: 0 2em;    margin-left:0;    max-width: 1200px;}blockquote:nth-of-type(10){    border-left:.5em solid #4114b4;    padding: 0 2em;    margin-left:0;    max-width: 1200px;}blockquote:nth-of-type(12){    border-left:.5em solid #4114b4;    padding: 0 2em;    margin-left:0;    max-width: 1200px;}blockquote:nth-of-type(14){    border-left:.5em solid #4114b4;    padding: 0 2em;    margin-left:0;    max-width: 1200px;}blockquote:nth-of-type(15){    border-left:.5em solid #4114b4;    padding: 0 2em;    margin-left:0;    max-width: 1200px;}blockquote:nth-of-type(17){    border-left:.5em solid #4114b4;    padding: 0 2em;    margin-left:0;    max-width: 1200px;}blockquote:nth-of-type(18){    border-left:.5em solid #4114b4;    padding: 0 2em;    margin-left:0;    max-width: 1200px;}blockquote:nth-of-type(21){    border-left:.5em solid #4114b4;    padding: 0 2em;    margin-left:0;    max-width: 1200px;}blockquote:nth-of-type(25){    border-left:.5em solid #4114b4;    padding: 0 2em;    margin-left:0;    max-width: 1200px;}blockquote  cite {    font-size:14px;    line-height:20px;    color:#bfbfbf;}blockquote cite:before {    content: '\2014 \00A0';}blockquote p {      color: #666;    max-width: 1200px;}hr {    height: 1px;    border: none;    border-top: 1px dashed #0066CC}button,input,select,textarea {  font-size: 100%;  margin: 0;  vertical-align: baseline;  *vertical-align: middle;}button, input {  line-height: normal;  *overflow: visible;}button::-moz-focus-inner, input::-moz-focus-inner {  border: 0;  padding: 0;}button,input[type="button"],input[type="reset"],input[type="submit"] {  cursor: pointer;  -webkit-appearance: button;}input[type=checkbox], input[type=radio] {  cursor: pointer;}/* override default chrome & firefox settings */input:not([type="image"]), textarea {  -webkit-box-sizing: content-box;  -moz-box-sizing: content-box;  box-sizing: content-box;}input[type="search"] {  -webkit-appearance: textfield;  -webkit-box-sizing: content-box;  -moz-box-sizing: content-box;  box-sizing: content-box;}input[type="search"]::-webkit-search-decoration {  -webkit-appearance: none;}label,input,select,textarea {  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;  font-size: 13px;  font-weight: normal;  line-height: normal;  margin-bottom: 18px;}input[type=checkbox], input[type=radio] {  cursor: pointer;  margin-bottom: 0;}input[type=text],input[type=password],textarea,select {  display: inline-block;  width: 210px;  padding: 4px;  font-size: 13px;  font-weight: normal;  line-height: 18px;  height: 18px;  color: #808080;  border: 1px solid #ccc;  -webkit-border-radius: 3px;  -moz-border-radius: 3px;  border-radius: 3px;}select, input[type=file] {  height: 27px;  line-height: 27px;}textarea {  height: auto;}/* grey out placeholders */:-moz-placeholder {  color: #bfbfbf;}::-webkit-input-placeholder {  color: #bfbfbf;}input[type=text],input[type=password],select,textarea {  -webkit-transition: border linear 0.2s, box-shadow linear 0.2s;  -moz-transition: border linear 0.2s, box-shadow linear 0.2s;  transition: border linear 0.2s, box-shadow linear 0.2s;  -webkit-box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1);  -moz-box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1);  box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1);}input[type=text]:focus, input[type=password]:focus, textarea:focus {  outline: none;  border-color: rgba(82, 168, 236, 0.8);  -webkit-box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1), 0 0 8px rgba(82, 168, 236, 0.6);  -moz-box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1), 0 0 8px rgba(82, 168, 236, 0.6);  box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1), 0 0 8px rgba(82, 168, 236, 0.6);}/* buttons */button {  display: inline-block;  padding: 4px 14px;  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;  font-size: 13px;  line-height: 18px;  -webkit-border-radius: 4px;  -moz-border-radius: 4px;  border-radius: 4px;  -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.2), 0 1px 2px rgba(0, 0, 0, 0.05);  -moz-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.2), 0 1px 2px rgba(0, 0, 0, 0.05);  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.2), 0 1px 2px rgba(0, 0, 0, 0.05);  background-color: #0064cd;  background-repeat: repeat-x;  background-image: -khtml-gradient(linear, left top, left bottom, from(#049cdb), to(#0064cd));  background-image: -moz-linear-gradient(top, #049cdb, #0064cd);  background-image: -ms-linear-gradient(top, #049cdb, #0064cd);  background-image: -webkit-gradient(linear, left top, left bottom, color-stop(0%, #049cdb), color-stop(100%, #0064cd));  background-image: -webkit-linear-gradient(top, #049cdb, #0064cd);  background-image: -o-linear-gradient(top, #049cdb, #0064cd);  background-image: linear-gradient(top, #049cdb, #0064cd);  color: #fff;  text-shadow: 0 -1px 0 rgba(0, 0, 0, 0.25);  border: 1px solid #004b9a;  border-bottom-color: #003f81;  -webkit-transition: 0.1s linear all;  -moz-transition: 0.1s linear all;  transition: 0.1s linear all;  border-color: #0064cd #0064cd #003f81;  border-color: rgba(0, 0, 0, 0.1) rgba(0, 0, 0, 0.1) rgba(0, 0, 0, 0.25);}button:hover {  color: #fff;  background-position: 0 -15px;  text-decoration: none;}button:active {  -webkit-box-shadow: inset 0 3px 7px rgba(0, 0, 0, 0.15), 0 1px 2px rgba(0, 0, 0, 0.05);  -moz-box-shadow: inset 0 3px 7px rgba(0, 0, 0, 0.15), 0 1px 2px rgba(0, 0, 0, 0.05);  box-shadow: inset 0 3px 7px rgba(0, 0, 0, 0.15), 0 1px 2px rgba(0, 0, 0, 0.05);}button::-moz-focus-inner {  padding: 0;  border: 0;}/* table  */table {    border-spacing: 0;    border: 1px solid #ccc;}td, th{    border: 1px solid #ccc;    padding: 5px;}/* code syntax highlight.Documentation: http://www.mdcharm.com/documentation/code_syntax_highlighting.html#custom_your_own */pre .literal,pre .comment,pre .template_comment,pre .diff .header,pre .javadoc {    color: #008000;}pre .keyword,pre .css .rule .keyword,pre .winutils,pre .javascript .title,pre .nginx .title,pre .subst,pre .request,pre .status {    color: #0000FF;    font-weight: bold}pre .number,pre .hexcolor,pre .python .decorator,pre .ruby .constant {    color: #0000FF;}pre .string,pre .tag .value,pre .phpdoc,pre .tex .formula {    color: #D14}pre .title,pre .id {    color: #900;    font-weight: bold}pre .javascript .title,pre .lisp .title,pre .clojure .title,pre .subst {    font-weight: normal}pre .class .title,pre .haskell .type,pre .vhdl .literal,pre .tex .command {    color: #458;    font-weight: bold}pre .tag,pre .tag .title,pre .rules .property,pre .django .tag .keyword {    color: #000080;    font-weight: normal}pre .attribute,pre .variable,pre .lisp .body {    color: #008080}pre .regexp {    color: #009926}pre .class {    color: #458;    font-weight: bold}pre .symbol,pre .ruby .symbol .string,pre .lisp .keyword,pre .tex .special,pre .prompt {    color: #990073}pre .built_in,pre .lisp .title,pre .clojure .built_in {    color: #0086b3}pre .preprocessor,pre .pi,pre .doctype,pre .shebang,pre .cdata {    color: #999;    font-weight: bold}pre .deletion {    background: #fdd}pre .addition {    background: #dfd}pre .diff .change {    background: #0086b3}pre .chunk {    color: #aaa}pre .markdown .header {    color: #800;    font-weight: bold;}pre .markdown .blockquote {    color: #888;}pre .markdown .link_label {    color: #88F;}pre .markdown .strong {    font-weight: bold;}pre .markdown .emphasis {    font-style: italic;}   </style>      </head><body><br><!-- <h1><strong>Xing SUN 孙星</strong></h1> --><!-- <p><strong>孙星</strong><br /> -->    <table border="0" width="80%"  frame=void  rules=none cellspacing="0" cellpadding="0" style="border:0;">    <tbody>      <tr>        <td style="border:0;"><img src="./images/sunxing.jpg"  frame=void  border="0" width="300"></td>        <td style="border:0;"><font size="3">            <strong>Xing SUN, Ph.D.</strong><br />            <br>            Team Lead & Senior Researcher<br />            <br>            <a href="https://youtu.qq.com/">YoutuLab, Tencent</a><br />            <br>            Email: <i>winfred.sun at gmail dot com</i><br />            <br>            Github: <a href="https://github.com/x-ing">https://github.com/X-ing</a></p>        </td>      </tr>    </tbody>  </table><!-- Ph.D.<br />Researcher<br />Tencent YoutuLab<br />Email: winfred.sun -at- gmail.com<br />Github: <a href="https://github.com/x-ing">https://github.com/X-ing</a></p><p><img src='./images/sunxing.jpg', width='300'></p> --><h2>News</h2><p>2021.07 One papers accepted to ACM MM 2021.<br />2021.04 Our light ReID framework <strong>YouReID</strong> is <a href="https://github.com/TencentYoutuResearch/PersonReID-CACENET">available</a>,  which implements our state-of-the-art ReID algorithms. <img src='./images/YouReID_Logo.png', width='70'> <br />2021.04 One papers accepted to IJCAI 2021.<br />2021.03 Three papers accepted to CVPR 2021.<br />2021.01 Our <a href="https://github.com/TencentYoutuResearch/PersonReID-CACENET">ReID project</a> has been merged into <a href="https://github.com/opencv/opencv">OpenCV</a>!<br />2021.01 One paper accepted to ICLR 2021.<br />2020.12 Two papers accepted to AAAI 2021.<br />2020.09 One paper accepted to NeurIPS 2020.<br />2020.07 One paper accepted to ECCV 2020.<br />2020.07 One paper accepted to ACM MM 2020.<br />2020.02 One paper accepted to CVPR 2020.<br />2019.11 Three papers accepted to AAAI 2020.<br />2019.10 One paper accepted by TPAMI.<br />2019.03 One paper accepted to CVPR 2019.</p><h2>Short Bio</h2><p>Xing Sun (孙星) is currently a team lead and senior researcher in Tencent YoutuLab. Before that, he received his Ph.D. degree under the supervision of <a href="http://www.eee.hku.hk/~elam/">Prof. Edmund Y. Lam</a> in Imaging Systems Laboratory, and <a href="http://www.its.hku.hk/research/projects/project/nyung.html">Dr. Nelson Yung</a> in Laboratory for Intelligent Transportation Systems Research in <a href="http://www.eee.hku.hk">the Department of Electrical and Electronic Engineering</a> at <a href="http://www.hku.hk">The University of Hong Kong</a> in 2016. He received his B.S. degree at <a href="http://www.njust.edu.cn/">Nanjing University of Science and Technology</a> in Jun. 2012. He finished my bachelor dissertation in <a href="http://www.hft.ei.tum.de/index.php?id=95">Lehrstuhl für Hochfrequenztechnik</a> from <a href="https://www.tum.de/en/">Technische Universität München</a> in Spring, 2012. </p><h2>Academic Activities</h2><ul><li>Senior PC member: IJCAI 2021</li><li>Conference reviewer: CVPR, ICCV, NeurIPS, ICLR, ACM MM, AAAI, IJCAI etc</li></ul><h2>Research Topics</h2><!-- --> <blockquote><p><strong>Human analysis</strong><br /> including Person ReID, Pedestrian Detection, Action Recognition etc</p></blockquote><!-- --> <blockquote><p><strong>Automtic AI</strong><br /> including AutoML, Unsupervised Learning, Self-supervised Learning etc</p></blockquote><h2>Recent Publications (<a href="https://scholar.google.com.hk/citations?user==IUtix9IAAAAJ&hl=en">Google Scholar</a>)</h2><p>(* Corresponding <sup>+</sup> Equal contribution)</p><strong>Preprint</strong> <!-- --> <blockquote><p>7. <a href="https://arxiv.org/pdf/2107.08391.pdf">AS-MLP: An Axial Shifted MLP Architecture for Vision</a> <br />Dongze Lian, Zehao Yu, <strong>Xing Sun</strong>, Shenghua Gao   <br />Arxiv Tech Report, 2021.<br /><a href="https://github.com/svip-lab/AS-MLP">Code is available!</a></p></blockquote><!-- --> <blockquote><p>6. <a href="https://arxiv.org/pdf/2104.09124v1.pdf">DisCo: Remedy Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning</a> <br />Yuting Gao, Jia-Xin Zhuang, Ke Li, Hao Cheng, Xiaowei Guo, Feiyue Huang, Rongrong Ji, and <strong>Xing Sun*</strong>  <br />Arxiv Tech Report, 2021.<br /><a href="https://github.com/Yuting-Gao/DisCo-pytorch">Code is available!</a></p></blockquote><!-- --> <blockquote><p>5. <a href="https://arxiv.org/pdf/2101.08237.pdf">On The Consistency Training for Open-Set Semi-Supervised Learning</a> <br />Huixiang Luo, Hao Cheng, Yuting Gao, Ke Li, Mengdan Zhang, Fanxu Meng, Xiaowei Guo, Feiyue Huang and <strong>Xing Sun*</strong>  <br />Arxiv Tech Report, 2021.<br /></p></blockquote><!-- --> <blockquote><p>4. <a href="https://arxiv.org/pdf/2103.13561v1.pdf">On Evolving Attention Towards Domain Adaptation</a> <br />Kekai Sheng, Ke Li, Xiawu Zheng, Jian Liang, WeiMing Dong, Feiyue Huang, Rongrong Ji, and <strong>Xing Sun*</strong>  <br />Arxiv Tech Report, 2021.<br /></p></blockquote><!-- --> <blockquote><p>3. <a href="https://arxiv.org/pdf/2103.01654.pdf">Part2Whole: Iteratively Enrich Detail for Cross-Modal Retrieval with Partial Query</a> <br />Guanyu Cai, Xinyang Jiang, Jun Zhang, Yifei Gong, Lianghua He, Pai Peng, Xiaowei Guo and <strong>Xing Sun*</strong>  <br />Arxiv Tech Report, 2021.<br /></p></blockquote><!-- --> <blockquote><p>2. <a href="https://arxiv.org/pdf/2101.03036">Contextual Non-Local Alignment over Full-Scale Representation for Text-Based Person Search</a> <br />Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng, Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo and <strong>Xing Sun*</strong>  <br />Arxiv Tech Report, 2021.<br />Ranked #1 on<a href="https://paperswithcode.com/sota/nlp-based-person-retrival-on-cuhk-pedes"> Text based Person Retrieval on CUHK-PEDES</a>！<br /><a href="https://github.com/TencentYoutuResearch/PersonReID-NAFS">Code is available!</a><img src='./images/YouReID_Logo.png', width='70'></p></blockquote><!-- --> <blockquote><p>1. <a href="https://arxiv.org/pdf/2009.05250">Devil's in the Detail: Aligning Visual Clues for Conditional Embedding in Person Re-Identification</a> <br />Fufu Yu, Xinyang Jiang, Yifei Gong, Shizhen Zhao, Xiaowei Guo, Wei-Shi Zheng and <strong>Xing Sun*</strong>  <br />Arxiv Tech Report, 2021.<br /><a href="https://github.com/TencentYoutuResearch/PersonReID-CACENET">Code is available!</a><img src='./images/YouReID_Logo.png', width='70'> </p></blockquote><strong>2021</strong> <!-- --> <blockquote><p>8. <a href="https://2021.acmmm.org/">Discriminator-free Generative Adversarial Attack</a> <br />Shaohao Lu, Yuqiao Xian, Ke Yan, Yi Hu, <strong>Xing Sun</strong> , Xiaowei Guo, Feiyue Huang, Weishi Zheng  <br />ACM Multimedia (MM), 2021. </p></blockquote><!-- --> <blockquote><p>7. <a href="https://ijcai-21.org/">Dig into Multi-modal Cues for Video Retrieval with Hierarchical Alignment</a> <br />Wenzhe Wang, Penghao Zhou, Runnan Chen, Mengdan Zhang, Guanyu Cai, Pai Peng, Xiaowei Guo, Jian Wu,  <strong>Xing Sun*</strong> <br />International Joint Conference on Artificial Intelligence (IJCAI), 2021. </p></blockquote><!-- --> <blockquote><p>6. <a href="https://arxiv.org/pdf/2009.05769">Removing the Background by Adding the Background: Towards Background Robust Self-supervised Video Representation Learning</a> <br />Jinpeng Wang, Yuting Gao, Ke Li, Yiqi Lin, Andy J Ma and <strong>Xing Sun*</strong> <br />IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. <br /><a href="https://fingerrec.github.io/index_files/jinpeng/papers/CVPR2021/project_website.html">Code is available!</a></p></blockquote><!-- --> <blockquote><p>5. <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Learning_3D_Shape_Feature_for_Texture-Insensitive_Person_Re-Identification_CVPR_2021_paper.pdf">Learning 3D Shape Feature for Texture-insensitive Person Re-identification</a> <br />Jiaxing Chen, Xinyang Jiang, Fudong Wang, Jun Zhang, Feng Zheng, <strong>Xing Sun</strong>, Weishi Zheng <br />IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.<br /> <a href="https://github.com/TencentYoutuResearch/PersonReID-CACENET">Code is coming soon!</a><img src='./images/YouReID_Logo.png', width='70'> </p></blockquote><!-- --> <blockquote><p>4. <a href="http://mftp.mmcheng.net/Papers/21CVPR-VideoSR.pdf">Temporal Modulation Network for Controllable Space-Time Video Super-Resolution</a><br />Gang Xu, Jun Xu, Zhen Li, Liang Wang, <strong>Xing Sun</strong>, Ming-Ming Cheng<br />IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.<br /><a href="https://github.com/CS-GangXu/TMNet">Code is available!</a></p></blockquote><!-- --> <blockquote><p>3. <a href="https://arxiv.org/pdf/2010.02347">Learning with Instance-Dependent Label Noise: A Sample Sieve Approach</a> <br />Hao Cheng, Zhaowei Zhu, Xingyu Li, Yifei Gong, <strong>Xing Sun</strong>, Yang Liu   <br />International Conference on Learning Representations (ICLR), 2021.<br /><a href="https://github.com/haochenglouis/cores">Code is available!</a></p></blockquote><!-- --> <blockquote><p>2. <a href="https://arxiv.org/pdf/2012.05475v2.pdf">One for More: Selecting Generalizable Samples for Generalizable ReID Model</a> <br />Enwei Zhang,Xinyang Jiang,Hao Cheng,Ancong Wu, Ke Li,Xiaowei Guo,Feng Zheng,Weishi Zheng and <strong>Xing Sun*</strong>  <br />The AAAI Conference on Artificial Intelligence (AAAI), 2021. <br /> <a href="https://github.com/TencentYoutuResearch/PersonReID-CACENET">Code is coming soon!</a> <img src='./images/YouReID_Logo.png', width='70'> </p></blockquote><!-- --> <blockquote><p>1. <a href="https://arxiv.org/pdf/2009.05757">Enhancing Unsupervised Video Representation Learning by Decoupling the Scene and the Motion</a> <br />Jinpeng Wang, Yuting Gao, Ke Li, Xinyang Jiang, Xiaowei Guo, Rongrong Ji and <strong>Xing Sun*</strong>  <br />The AAAI Conference on Artificial Intelligence (AAAI), 2021. <br /><a href="https://github.com/TencentYoutuResearch/SelfSupervisedLearning-DSM">Code is available!</a></p></blockquote><strong>2020</strong> <!-- --> <blockquote><p>7. <a href="https://arxiv.org/abs/2009.14410">Pruning Filter in Filter</a> <br />Fanxu Meng, Hao Cheng, Ke Li, Huixiang Luo, Xiaowei Guo, Guangming Lu and <strong>Xing Sun*</strong> <br />Neural Information Processing Systems (NeurIPS), 2020. <br /><a href="https://github.com/TencentYoutuResearch/Pruning-PFF">Code is available!</a></p></blockquote><!-- --> <blockquote><p>6. <a href="https://arxiv.org/pdf/2008.06963">Do Not Disturb Me: Person Re-identification Under the Interference of Other Pedestrians</a> <br />Shizhen Zhao, Changxin Gao, Jun Zhang, Hao Cheng, Chuchu Han, Xinyang Jiang, XW Guo, WS Zheng, Nong Sang, <strong>Xing Sun</strong> <br />European Conference on Computer Vision (ECCV), 2020. <br /><a href="https://github.com/X-BrainLab/PI-ReID">Code is available!</a></p></blockquote><!-- --> <blockquote><p>5. <a href="https://dl.acm.org/doi/pdf/10.1145/3394171.3413617">NOH-NMS: Improving Pedestrian Detection by Nearby Objects Hallucination</a> <br />Penghao Zhou, Chong Zhou, Pai Peng, Junlong Du, <strong>Xing Sun</strong>, Xiaowei Guo, Feiyue Huang <br />ACM Multimedia (MM), 2020. <br /><a href="https://github.com/TencentYoutuResearch/PedestrianDetection-NohNMS">Code is available!</a></p></blockquote><!-- --> <blockquote><p>4. <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Meng_Filter_Grafting_for_Deep_Neural_Networks_CVPR_2020_paper.html">Filter Grafting for Deep Neural Networks</a> <br />Fanxu Meng<sup>+</sup> , Hao Cheng<sup>+</sup> , Ke Li, Zhixin Xu, Rongrong Ji, <strong>Xing Sun</strong> and Guangming Lu  <br />IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. <br /><a href="https://github.com/TencentYoutuResearch/Ensemble-Grafting">Code is available!</a></p></blockquote><!-- --> <blockquote><p>3. <a href="https://arxiv.org/pdf/1912.01349.pdf">Asymmetric Co-Teaching for Unsupervised Cross-Domain Person Re-Identification</a> <br />Fengxiang Yang, Ke Li, Zhun Zhong, Zhiming Luo,  <strong>Xing Sun*</strong>, Hao Cheng, Xiaowei Guo, Feiyue Huang, Rongrong Ji, Shaozi Li  <br />The AAAI Conference on Artificial Intelligence (AAAI), 2020. <br /><a href="https://github.com/TencentYoutuResearch/PersonReID-ACT">Code is available!</a> <img src='./images/YouReID_Logo.png', width='70'> </p></blockquote><!-- --> <blockquote><p>2. <a href="https://arxiv.org/pdf/1912.01300">Aware Loss with Angular Regularization for Person Re-Identification</a> <br />Zhihui Zhu, Xinyang Jiang, Feng Zheng, Xiaowei Guo, Feiyue Huang, <strong>Xing Sun*</strong>, Weishi Zheng <br />The AAAI Conference on Artificial Intelligence (AAAI), 2020. <br /><a href="https://github.com/zzhsysu/VA-ReID">Code is coming soon!</a></p></blockquote><!-- --> <blockquote><p>1. <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6770/6624">Rethinking Temporal Fusion for Video-Based Person Re-Identification on Semantic and Time Aspect</a> <br />Xinyang Jiang, Yifei Gong, Xiaowei Guo, Qize Yang, Feiyue Huang, Weishi Zheng, Feng Zheng and <strong>Xing Sun*</strong> <br />The AAAI Conference on Artificial Intelligence (AAAI), 2020.(<strong>Oral</strong>) <br /><a href="https://github.com/TencentYoutuResearch/PersonReID-TSF">Code is available!</a></p></blockquote><strong>2019</strong> <!-- --> <blockquote><p>2. <a href="https://arxiv.org/pdf/1910.01426.pdf">High-dimensional dense residual convolutional neural network for light field reconstruction</a> <br />Nan Meng, Hayden Kwok-Hay So, <strong>Xing Sun</strong> and Edmund Lam  <br />IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2019. <br /><a href="https://github.com/monaen/LightFieldReconstruction">Code is available!</a></p></blockquote><!-- --> <blockquote><p>1. <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zheng_Pyramidal_Person_Re-IDentification_via_Multi-Loss_Dynamic_Training_CVPR_2019_paper.pdf">Pyramidal Person Re-IDentification via Multi-Loss Dynamic Training</a> <br />Feng Zheng, Cheng Deng, <strong>Xing Sun*</strong> , Xinyang Jiang, Zongqiao Yu, Feiyue Huang, Xiaowei Guo and Rongrong Ji  <br />IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. <br /> <a href="https://github.com/TencentYoutuResearch/PersonReID-CACENET">Code is available!</a><img src='./images/YouReID_Logo.png', width='70'> </p></blockquote><h2>Competition</h2><ul><li>1st place in CrowdHuman 2020</li></ul></body></html>