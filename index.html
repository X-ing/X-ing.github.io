<!DOCTYPE html><html lang="en"><head>    <meta charset="UTF-8">    <meta name="viewport" content="width=device-width, initial-scale=1.0">    <title>Xing SUN (孙星)</title>    <style>        :root {            --primary-color: #0969da; /* GitHub Blue */            --text-color: #1f2328;            --light-text: #656d76;            --bg-color: #ffffff;            --section-bg: #f6f8fa;            --border-color: #d0d7de;            --tag-bg: #eff1f3;            --tag-text: #1f2328;        }        body{font-family:Georgia,'Times New Roman',serif;max-width:900px;margin:40px auto;line-height:1.7;color:#111;}        a {            color: var(--primary-color);            text-decoration: none;        }        a:hover {            text-decoration: underline;        }        /* Header */        header {            display: flex;            align-items: center;            gap: 32px;            margin-bottom: 48px;            padding-bottom: 32px;            border-bottom: 1px solid var(--border-color);        }                .profile-img {            width: 140px;            height: 140px;            border-radius: 50%;            object-fit: cover;            border: 1px solid var(--border-color);            box-shadow: 0 1px 3px rgba(0,0,0,0.1);        }        .profile-info h1 {            font-size: 2rem;            margin: 0 0 8px 0;            font-weight: 600;        }                .profile-meta {            color: var(--light-text);            font-size: 1rem;            margin-bottom: 12px;        }        .social-links {            display: flex;            gap: 16px;            font-size: 0.9rem;        }        .social-links a {            color: var(--light-text);            display: flex;            align-items: center;            gap: 4px;        }        .social-links a:hover {            color: var(--primary-color);        }        /* Standard Section */        section {            margin-bottom: 48px;        }        h2 {            font-size: 1.5rem;            margin-bottom: 24px;            padding-bottom: 8px;            border-bottom: 1px solid var(--border-color);            font-weight: 600;        }        /* Category Sub-headers */        h3.category-title {            font-size: 1.1rem;            color: var(--text-color);            margin-top: 32px;            margin-bottom: 16px;            font-weight: 600;            display: flex;            align-items: center;        }                h3.category-title::before {            content: '';            display: inline-block;            width: 4px;            height: 16px;            background-color: var(--primary-color);            margin-right: 10px;            border-radius: 2px;        }        /* Open Source Grid */        .grid-container {            display: grid;            grid-template-columns: repeat(auto-fill, minmax(280px, 1fr));            gap: 16px;        }        .repo-card {            border: 1px solid var(--border-color);            border-radius: 6px;            padding: 16px;            background: #fff;            transition: all 0.2s ease;        }        .repo-card:hover {            border-color: var(--primary-color);            box-shadow: 0 2px 8px rgba(0,0,0,0.05);        }        .repo-card h4 {            margin: 0 0 8px 0;            font-size: 1rem;        }        .repo-card p {            font-size: 0.85rem;            color: var(--light-text);            margin: 0 0 12px 0;            line-height: 1.4;        }        .repo-tags {            display: flex;            gap: 8px;            flex-wrap: wrap;        }        .tag {            background-color: var(--tag-bg);            color: var(--tag-text);            font-size: 0.75rem;            padding: 2px 8px;            border-radius: 12px;            font-weight: 500;        }        /* Publication List */        ul.pub-list {            list-style: none;            padding: 0;        }        .tag.agent { background-color: #e6ffec; color: #1a7f37; }         .tag.rag { background-color: #fff8c5; color: #9a6700; }         .tag.mllm { background-color: #fbefff; color: #8250df; }        /* Category Headers */        .category-header {            font-size: 1.2rem;            font-weight: 600;            color: #24292f;            margin-top: 40px;            margin-bottom: 20px;            padding-left: 12px;            border-left: 4px solid var(--primary-color);            display: flex;            align-items: center;            justify-content: space-between;        }                li.pub-item {            margin-bottom: 16px;            padding-bottom: 16px;            border-bottom: 1px dashed #eee;        }        li.pub-item:last-child {            border-bottom: none;        }        .pub-title {            font-weight: 600;            color: #111;            font-size: 1rem;            display: inline;        }                .pub-venue {            font-size: 0.9rem;            color: #57606a;            display: block;            margin-top: 4px;        }        .badge {            display: inline-block;            font-size: 0.7rem;            padding: 1px 6px;            border-radius: 4px;            margin-left: 6px;            font-weight: 600;            vertical-align: middle;        }        .badge-new { background: #cf222e; color: #fff; }        .badge-spotlight { background: #bf8700; color: #fff; }        /* Responsive */        @media (max-width: 600px) {            header { flex-direction: column; text-align: center; }            .profile-img { margin-bottom: 16px; }            .social-links { justify-content: center; }        }    </style></head><body>    <header>        <img src="./images/sunxing.jpg" alt="Xing SUN" class="profile-img">        <div class="profile-info">            <h1>Xing SUN (孙星)</h1>            <div class="profile-meta">                Ph.D., Principal Researcher & Team Manager<br>                <a href="https://www.tencent.com/en-us/">Tencent Youtu Lab</a>            </div>            <div class="social-links">                <a href="mailto:winfred.sun@gmail.com">Email</a>                <a href="https://scholar.google.com.hk/citations?hl=en&user=IUtix9IAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Google Scholar</a>                <a href="https://huggingface.co/collections/tencent/youtu" target="_blank">HuggingFace</a>                 <a href="https://www.linkedin.com/in/sunxings/" target="_blank">Linkedin</a>            </div>        </div>    </header>    <section>        <h2>About Me</h2>        <p>            I am a Principal Researcher and Team Manager at Tencent Youtu Lab. I received my Ph.D. from The University of Hong Kong in 2016.        </p>        <p>            I dedicate my effort to three core pillars: <strong>Agents</strong>, <strong>Retrieval-Augmented Generation (RAG)</strong>, and <strong>Multimodal Large Language Models (MLLM)</strong>. We aim to bridge the gap between foundation models and real-world applications through robust, open-source tools and benchmarks.        </p>    </section>    <section>        <h2>Open Source Projects</h2>        <p style="font-size: 0.9rem; margin-bottom: 20px; color: var(--light-text);">            We are actively building the <a href="https://github.com/TencentCloudADP">TencentCloudADP</a> ecosystem.        </p>        <h3 class="category-title">Agents</h3>        <div class="grid-container">            <div class="repo-card">                <h4><a href="https://huggingface.co/tencent/Youtu-LLM-2B">Youtu-LLM</a></h4>                <p>Lightweight, high-performance Large Language Models (2B parameters) for edge deployment.</p>                <div class="repo-tags"><span class="tag">LLM</span><span class="tag">HuggingFace</span></div>            </div>            <div class="repo-card">                <h4><a href="https://github.com/TencentCloudADP/youtu-agent">Youtu-Agent</a></h4>                <p>A flexible framework for building autonomous LLM agents, supporting complex tool calling, planning, and memory management.</p>                <div class="repo-tags"><span class="tag">Framework</span><span class="tag">Python</span></div>            </div>            <div class="repo-card">                <h4><a href="https://github.com/TencentCloudADP/youtu-tip">Youtu-Tip</a></h4>                <p>A desktop efficiency assistant powered by local LLMs (Ollama) and Youtu-Agent to automate daily workflows.</p>                <div class="repo-tags"><span class="tag">Application</span><span class="tag">Local LLM</span></div>            </div>        </div>        <h3 class="category-title">RAG</h3>        <div class="grid-container">            <div class="repo-card">                <h4><a href="https://github.com/TencentCloudADP/youtu-graphrag">Youtu-GraphRAG</a></h4>                <p>Advanced RAG system leveraging Knowledge Graphs to enhance retrieval accuracy and structured reasoning.</p>                <div class="repo-tags"><span class="tag">RAG</span><span class="tag">Graph</span></div>            </div>            <div class="repo-card">                <h4><a href="https://github.com/TencentCloudADP/youtu-parsing">Youtu-Parsing</a></h4>                <p>High-performance document parsing tools designed to convert raw files (PDF, Docx) into clean RAG-ready data.</p>                <div class="repo-tags"><span class="tag">Data Processing</span></div>            </div>            <div class="repo-card">                <h4><a href="https://github.com/TencentCloudADP/youtu-embedding">Youtu-Embedding</a></h4>                <p>Optimized embedding models tailored for semantic search and dense retrieval tasks.</p>                <div class="repo-tags"><span class="tag">Model</span><span class="tag">Retrieval</span></div>            </div>        </div>        <h3 class="category-title">MLLM</h3>        <div class="grid-container">            <div class="repo-card">                <h4><a href="https://github.com/MME-Benchmarks/Video-MME">Video-MME</a></h4>                <p>The first-ever comprehensive evaluation benchmark of multi-modal LLMs in video analysis.</p>                <div class="repo-tags"><span class="tag">Benchmark</span><span class="tag">Video</span></div>            </div>            <div class="repo-card">                <h4><a href="https://github.com/TencentCloudADP/youtu-vl">Youtu-VL</a></h4>                <p>Open-source Vision-Language models including training recipes and inference code (e.g., Youtu-VL-4B).</p>                <div class="repo-tags"><span class="tag">MLLM</span><span class="tag">Training</span></div>            </div>            <div class="repo-card">                <h4><a href="https://github.com/VITA-MLLM/VITA">VITA</a></h4>                <p>The first-ever open-source interactive omni-multimodal LLM</p>                <div class="repo-tags"><span class="tag">MLLM</span><span class="tag">Training</span></div>            </div>        </div>    </section>    <section>        <h2>Selected Publications</h2>        <p style="font-size: 0.9rem; color: #666; margin-bottom: 20px;">            Full list available on <a href="https://scholar.google.com.hk/citations?hl=en&user=IUtix9IAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a>.        </p>        <!-- Topic 1: Autonomous Agents -->        <div class="category-header">            Agents        </div>        <ul class="pub-list">            <li class="pub-item">                <span class="pub-title">Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning</span>                <span class="badge badge-new">ICLR 2026</span>            </li>            <li class="pub-item">                <span class="pub-title">Process-Level Trajectory Evaluation for Environment Configuration in Software Engineering Agents</span>                <span class="badge badge-new">ICLR 2026</span>            </li>            <li class="pub-item">                <span class="pub-title">RolePlot: A Systematic Framework for Evaluating and Enhancing the Plot-Progression Capabilities of Role-Playing Agents</span>                <span class="badge badge-new">ACL 2025</span>            </li>            <li class="pub-item">                <span class="pub-title">Tell Me What You Don’t Know: Enhancing Refusal Capabilities of Role-Playing Agents via Representation Space Analysis and Editing</span>                <span class="badge badge-new">ACL 2025</span>            </li>            <li class="pub-item">                <span class="pub-title">RoleMRC: A Fine-Grained Composite Benchmark for Role-Playing and Instruction-Following</span>                <span class="badge badge-new">ACL 2025</span>            </li>            <li class="pub-item">                <span class="pub-title">TransMLA: Migrating GQA Models to MLA with Full DeepSeek Compatibility and Speedup</span>                <span class="badge badge-new">NeurIPS 2025</span> <span class="badge badge-spotlight">Spotlight</span>            </li>            <li class="pub-item">                <span class="pub-title">MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL</span>                <span class="badge badge-new">COLING 2025</span>            </li>        </ul>        <!-- Topic 2: Reasoning & RAG -->        <div class="category-header">            RAG        </div>        <ul class="pub-list">            <li class="pub-item">                <span class="pub-title">Youtu-GraphRAG: Vertically Unified Agents for Graph Retrieval-Augmented Complex Reasoning</span>                <span class="badge badge-new">ICLR 2026</span>            </li>            <li class="pub-item">                <span class="pub-title">Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards</span>                <span class="badge badge-new">ICLR 2026</span>            </li>            <li class="pub-item">                <span class="pub-title">Attend to the Active: Structure-Aware Dynamic Attention in LLMs for Compositional Instruction Following</span>                <span class="badge badge-new">ICLR 2026</span>            </li>             <li class="pub-item">                <span class="pub-title">Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving</span>                <span class="badge badge-new">AAAI 2026</span>            </li>            </li>            <li class="pub-item">                <span class="pub-title">Sequential-NIAH: A Needle-In-A-Haystack Benchmark for Extracting Sequential Needles from Long Contexts</span>                <span class="badge badge-new">EMNLP 2025</span>            </li>            <li class="pub-item">                <span class="pub-title">HRVDA: High-Resolution Visual Document Assistant</span>                <span class="badge badge-new">CVPR 2024</span>            </li>        </ul>        <!-- Topic 3: Multimodal LLM -->        <div class="category-header">            MLLM        </div>        <ul class="pub-list">             <li class="pub-item">                <span class="pub-title">RAR: Reversing Visual Attention Re-Sinking for Unlocking Potential in Multimodal Large Language Models</span>                <span class="badge badge-new">ICLR 2026</span>            </li>            <li class="pub-item">                <span class="pub-title">VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction</span>                <span class="badge badge-new">NeurIPS 2025</span> <span class="badge badge-spotlight">Spotlight</span>            </li>            <li class="pub-item">                <span class="pub-title">MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models</span>                <span class="badge badge-new">NeurIPS 2025</span> <span class="badge badge-spotlight">Spotlight</span>            </li>            <li class="pub-item">                <span class="pub-title">VITA-Audio: Fast Interleaved Audio-Text Token Generation for Efficient Large Speech-Language Model</span>                <span class="badge badge-new">NeurIPS 2025</span>            </li>            <li class="pub-item">                <span class="pub-title">Zooming from Context to Cue: Hierarchical Preference Optimization for Multi-Image MLLMs</span>                <span class="badge badge-new">NeurIPS 2025</span>            </li>            <li class="pub-item">                <span class="pub-title">LTD-Bench: Evaluating Large Language Models by Letting Them Draw</span>                <span class="badge badge-new">NeurIPS 2025</span>            </li>            <li class="pub-item">                <span class="pub-title">Learning Interleaved Image-Text Comprehension in Vision-Language Large Models</span>                <span class="badge badge-new">ICLR 2025</span>            </li>            <li class="pub-item">                <span class="pub-title">Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis</span>                <span class="badge badge-new">CVPR 2025</span><span class="badge badge-spotlight">Highlight</span>            </li>        </ul>    </section>    <footer style="margin-top: 60px; padding-top: 24px; border-top: 1px dashed var(--border-color); text-align: center; font-size: 0.85rem; color: var(--light-text);">        &copy; 2026 Xing SUN. Last updated: Feb 2026.    </footer></body></html>